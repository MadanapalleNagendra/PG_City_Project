{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc58f2c9-70c3-4e21-936f-1b2071ee3c71",
   "metadata": {},
   "source": [
    "# bangalore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1abb16fb-1fe9-46c9-b564-7a081594bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "for page in range(1,161):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-bangalore-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"roominfo-data-parent\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__tuple__flex\"):\n",
    "    #     near.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72a64aa3-b62b-45d8-b1d0-18fcb4c7d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "RCB=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "RCB['City_name'] = 'Bangalore'\n",
    "\n",
    "RCB.to_csv(\"C://New_PG_Project//New_PG_Project_RCB_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01776aac-cf84-402b-a747-da2ac0a58f82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fec4251-5241-4ffc-8985-537cacef937f",
   "metadata": {},
   "source": [
    "# Hyderabad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66201d61-e6bf-4447-8ac1-850239bf35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "Title=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "Type=[]\n",
    "gender=[]\n",
    "location=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "near=[]\n",
    "\n",
    "\n",
    "for page in range(1,40):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-hyderabad-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "        Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\", class_=\"m-srp-card clearfix\"):\n",
    "    #     text = i.find(\"div\", class_=\"m-srp-card__tuple__flex\")\n",
    "    #     if text:\n",
    "    #         vk = text.text.strip() \n",
    "    #         near.append(vk if vk else np.nan) \n",
    "    #     else:\n",
    "    #         near.append(np.nan) \n",
    "        \n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfb11d2a-c6a1-4683-a834-6281c8a8c643",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRH=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "SRH['City_name'] = 'Hyderabad'\n",
    "\n",
    "SRH.to_csv(\"C://New_PG_Project//New_PG_Project_SRH_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ec2d77-2b9f-40d7-be2f-d37a45b5dc1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d411d4c-c1cf-4b6b-a60b-2998729b14da",
   "metadata": {},
   "source": [
    "# Chennai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06c67204-befa-49f3-a719-94c07a7ad6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "for page in range(1,40):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-chennai-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"roominfo-data-parent\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__tuple__flex\"):\n",
    "    #     near.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "871593df-b047-43b3-959b-0c299bb7837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSK=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "CSK['City_name'] = 'Chennai'\n",
    "\n",
    "CSK.to_csv(\"C://New_PG_Project//New_PG_Project_CSK_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e27a3-559f-4c2b-af17-a319c5dbe6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76683d40-8964-4909-9c4d-caeb5ad0efd4",
   "metadata": {},
   "source": [
    "# Ahmedabad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "075d76ac-165a-483a-b40d-ab9ae577ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "for page in range(1,30):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-ahmedabad-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__tuple__flex\"):\n",
    "    #     near.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c98dcdc0-a7f4-4b05-b91c-c27ece19fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "GT['City_name'] = 'Ahmedabad'\n",
    "\n",
    "GT.to_csv(\"C://New_PG_Project//New_PG_Project_GT_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d017a0b4-158e-4355-a9fa-aa4f8af3cd93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ccdedcc-9e2c-4011-aaed-19b9775cf844",
   "metadata": {},
   "source": [
    "# Nodia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6e4595f-913d-4d5a-9fb5-1da8a3b58ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "for page in range(1,47):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-noida-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__tuple__flex\"):\n",
    "    #     near.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "352d0801-355a-4925-a346-2c626e835f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nodia=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "Nodia['City_name'] = 'Nodia'\n",
    "\n",
    "Nodia.to_csv(\"C://New_PG_Project//New_PG_Project_Nodia_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c99ae5-1d2b-45a5-9cea-2a090fc9a1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3a6d36b-c418-4b3d-a4ab-dab9c18d9813",
   "metadata": {},
   "source": [
    "# faridabad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "745b0c53-1b06-40ab-b31d-a30f8c181a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "for page in range(1,6):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-faridabad-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card clearfix\"):\n",
    "    #     text=i.find(\"div\",class_=\"m-srp-card__tuple__flex\")\n",
    "    #     if text:\n",
    "    #         near.append(text.text)\n",
    "    #     else:\n",
    "    #         near.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd519bc2-7ffe-4d42-bb10-7ba8f466b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Faridabad=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "Faridabad['City_name'] = 'Faridabad'\n",
    "\n",
    "Faridabad.to_csv(\"C://New_PG_Project//New_PG_Project_Faridabad_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c216f8f-2884-449d-b1f4-719b9f574cb7",
   "metadata": {},
   "source": [
    "#  Mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "514cf85a-e016-45ff-a90d-b437b5467bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "for page in range(1,72):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-mumbai-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__tuple__flex\"):\n",
    "    #     near.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "58b1b458-6d11-4d5b-85bb-157799143e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "MI=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "MI['City_name'] = 'Mumbai'\n",
    "\n",
    "MI.to_csv(\"C://New_PG_Project//New_PG_Project_Mumbai_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a54f7-33ea-4762-8865-ed18a23bcb6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a0a7fa9-e7d1-457e-9b16-12088f754b6a",
   "metadata": {},
   "source": [
    "# Gurgaon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b15bb18d-96df-4387-ae95-90b04843a0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1,77):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-gurgaon-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__tuple__flex\"):\n",
    "    #     near.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "82e33464-42b6-4829-ad2e-72a918bf8f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gurgaon=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "Gurgaon['City_name'] = 'Gurgaon'\n",
    "\n",
    "Gurgaon.to_csv(\"C://New_PG_Project//New_PG_Project_Gurgaon_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b1b77-739c-4c78-bb14-e38cc6b8fdb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "634645e2-c240-4fa4-98c7-3991ec9f02c0",
   "metadata": {},
   "source": [
    "# Pune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a0ab58e3-b978-4ecf-bc20-d2a1a9d99995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1,86):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-pune-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__tuple__flex\"):\n",
    "    #     near.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e982c33f-287f-4a97-a47a-faffd04d6a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pune=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "Pune['City_name'] = 'Pune'\n",
    "\n",
    "Pune.to_csv(\"C://New_PG_Project//New_PG_Project_Pune_All_Pages.csv\", encoding='utf-8', errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9bc330-7724-4cb9-84ef-e97246740254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eec6db53-b8a8-48c8-9357-ac817f0392c8",
   "metadata": {},
   "source": [
    "# Thane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "863d0d1c-ba4b-474f-b5b8-6290a94b3a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1,8):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-thane-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__tuple__flex\"):\n",
    "    #     near.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ccf87c6-fd08-4410-9e95-5336904817e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Thane=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "Thane['City_name'] = 'Thane'\n",
    "\n",
    "Thane.to_csv(\"C://New_PG_Project//New_PG_Project_Thane_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f46c9a-ef69-4ab1-b4d9-3e1218485a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88a4d078-3132-4a18-8231-1a2ab7a85986",
   "metadata": {},
   "source": [
    "# ghaziabad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18fd8569-4906-44d1-b747-b4262a494b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1,15):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-ghaziabad-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__tuple__flex\"):\n",
    "    #     near.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91972081-ff96-4a8a-8373-400b3f55ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ghaziabad=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "ghaziabad['City_name'] = 'ghaziabad'\n",
    "\n",
    "ghaziabad.to_csv(\"C://New_PG_Project//New_PG_Project_ghaziabad_All_Pages.csv\", encoding='utf-8', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53aa1a4-6f2a-4dfa-8a9d-934864198903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c01835dd-d6ff-4ffa-83aa-90060d37db94",
   "metadata": {},
   "source": [
    "# NEW_DELHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c50e604-a2a9-41eb-a238-c47d6d48bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1,115):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-new-delhi-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__tuple__flex\"):\n",
    "        near.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ef402bc-8ad3-4d9e-a72d-da0af031a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "DC=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "DC['City_name'] = 'NEW DELHI'\n",
    "\n",
    "DC.to_csv(\"C://New_PG_Project//New_PG_Project_DC_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6934fbd4-4aa4-4687-b35d-55fdce27368f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b00e898-3ab8-47cd-b1ea-a85109473b9a",
   "metadata": {},
   "source": [
    "# Agra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "191f534d-c80d-42ad-86ab-3c897e1e3549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1,4):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-agra-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__tuple__flex\"):\n",
    "    #     near.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c29d9dd3-5620-428b-8ced-bd967cdb7746",
   "metadata": {},
   "outputs": [],
   "source": [
    "Agra=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "Agra['City_name'] = 'AGRA'\n",
    "\n",
    "Agra.to_csv(\"C://New_PG_Project//New_PG_Project_AGRA_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db49634a-c351-4e24-8536-c0d3c4c04e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e41e069-2a54-404f-b387-497e65a89913",
   "metadata": {},
   "source": [
    "# Kochi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53338211-cf22-4848-a593-f010e7bc5fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1,3):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-kochi-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card clearfix\"):\n",
    "    #     text=i.find(\"div\",class_=\"m-srp-card__tuple__flex\")\n",
    "    #     if text:\n",
    "    #         near.append(text.text)\n",
    "    #     else:\n",
    "    #         near.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9a937e0-f418-4185-9fb3-e15e9361fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kochi=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "Kochi['City_name'] = 'Kochi'\n",
    "\n",
    "Kochi.to_csv(\"C://New_PG_Project//New_PG_Project_Kochi_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa37b1-791b-4610-a2c2-af9c752be9be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7713d254-da9a-4ad4-964c-3ccd6bd9d4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2a8256d-3025-477c-8769-36814d2b8d79",
   "metadata": {},
   "source": [
    "# Trichy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aae7c0e3-2b86-4198-aa05-d4ed105a4ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1,2):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-trichy-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card clearfix\"):\n",
    "    #     text=i.find(\"div\",class_=\"m-srp-card__tuple__flex\")\n",
    "    #     if text:\n",
    "    #         near.append(text.text)\n",
    "    #     else:\n",
    "    #         near.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b89099ce-626f-46bd-8fcd-13eafbff0979",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trichy=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "Trichy['City_name'] = 'Trichy'\n",
    "\n",
    "Trichy.to_csv(\"C://New_PG_Project//New_PG_Project_Trichy_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8cb4a7-e640-463b-8bb8-ad843dddc025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b1c6dcc-1909-4ea8-bd4a-a04295c6b97d",
   "metadata": {},
   "source": [
    "# Allahabad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91d79223-1fec-406f-a44b-796364da6dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1,5):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-allahabad-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__tuple__flex\"):\n",
    "    #     near.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b9dcc3a-acc4-42bc-b685-80cf4c5563ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Allahabad=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "Allahabad['City_name'] = 'Allahabad'\n",
    "\n",
    "Allahabad.to_csv(\"C://New_PG_Project//New_PG_Project_Allahabad_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a01c51d-abe5-4de2-b384-7cc804d5d4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7752fc9a-9d0f-4472-af2e-1006eeb4ef45",
   "metadata": {},
   "source": [
    "# Goa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8637161b-107a-4ed9-b4a6-192a851bd5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1,3):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-goa-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card clearfix\"):\n",
    "    #     text=i.find(\"div\",class_=\"m-srp-card__tuple__flex\")\n",
    "    #     if text:\n",
    "    #         near.append(text.text)\n",
    "    #     else:\n",
    "    #         near.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4aec35b8-80a5-4939-bd49-9d5422811f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Goa=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "Goa['City_name'] = 'Goa'\n",
    "\n",
    "Goa.to_csv(\"C://New_PG_Project//New_PG_Project_Goa_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeccfd29-ad22-41c1-be00-8e5e27b9354a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "833c36ef-8c1f-4754-bb5a-75e1a1db191c",
   "metadata": {},
   "source": [
    "# patna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab2f941f-72c3-4ec5-960f-ebec3ac010cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1,8):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-patna-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card clearfix\"):\n",
    "        text=i.find(\"div\",class_=\"m-srp-card__tuple__flex\")\n",
    "        if text:\n",
    "            near.append(text.text)\n",
    "        else:\n",
    "            near.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b282ce8b-9e83-4ea9-82c7-30e0e8db4adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Patna=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "Patna['City_name'] = 'Patna'\n",
    "\n",
    "Patna.to_csv(\"C://New_PG_Project//New_PG_Project_Patna_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeeab1f-a2a3-4c1c-9049-b96d0bbf2ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ece2469c-26c2-4f25-b8f0-9b66c72a2079",
   "metadata": {},
   "source": [
    "# udupi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f5ccbc78-12f3-4272-94c2-a1cc85900522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1,2):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-udupi-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card clearfix\"):\n",
    "    #     text=i.find(\"div\",class_=\"m-srp-card__tuple__flex\")\n",
    "    #     if text:\n",
    "    #         near.append(text.text)\n",
    "    #     else:\n",
    "    #         near.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af85af98-5573-4b09-83af-96cd95afe189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7f7d95a5-ad4f-4bbb-9286-1146cfc5afa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "udupi=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "udupi['City_name'] = 'udupi'\n",
    "\n",
    "udupi.to_csv(\"C://New_PG_Project//New_PG_Project_udupi_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a1bfff-f7aa-4f6f-948b-6e558c9ffb0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1ea240-454c-4f49-8332-5c27ff0b9672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "727c24d8-b190-4896-9f35-a102ec4d8e85",
   "metadata": {},
   "source": [
    "# Lucknow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6fd8425a-0b0d-416d-b449-f2f36ff12778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "for page in range(1,26):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-lucknow-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card clearfix\"):\n",
    "    #     text=i.find(\"div\",class_=\"m-srp-card__tuple__flex\")\n",
    "    #     if text:\n",
    "    #         near.append(text.text)\n",
    "    #     else:\n",
    "    #         near.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "37c6410d-8e92-4e43-9579-6f2643d29fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lucknow=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "Lucknow['City_name'] = 'Lucknow'\n",
    "\n",
    "Lucknow.to_csv(\"C://New_PG_Project//New_PG_Project_Lucknow_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f171b2-6481-42b2-bffc-6c77d0946312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c58590a8-4216-4444-9829-f5a4a793456e",
   "metadata": {},
   "source": [
    "# Ranchi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "68584a30-2a7a-456a-b48e-dc1f0d49c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1,8):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-ranchi-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card clearfix\"):\n",
    "    #     text=i.find(\"div\",class_=\"m-srp-card__tuple__flex\")\n",
    "    #     if text:\n",
    "    #         near.append(text.text)\n",
    "    #     else:\n",
    "    #         near.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "de51912a-f45a-4f6b-9609-4a9efe96c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ranchi=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "Ranchi['City_name'] = 'Ranchi'\n",
    "\n",
    "Ranchi.to_csv(\"C://New_PG_Project//New_PG_Project_Ranchi_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282c0978-5ec8-464d-a996-142cfd292cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e02ae7-9f62-4f22-8949-d7b9573849b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2698dd13-d99c-43eb-a2b6-8ebb97401f2f",
   "metadata": {},
   "source": [
    "# Madurai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e9b5743f-496b-4272-8d0b-399bc121995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "for page in range(1,2):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-madurai-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card clearfix\"):\n",
    "        text=i.find(\"div\",class_=\"m-srp-card__tuple__flex\")\n",
    "        if text:\n",
    "            near.append(text.text)\n",
    "        else:\n",
    "            near.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "23364783-0cd1-47b8-9dbf-75931dff78de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Madurai=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "Madurai['City_name'] = 'Madurai'\n",
    "\n",
    "Madurai.to_csv(\"C://New_PG_Project//New_PG_Project_Madurai_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d49b720-04ea-4b42-b632-2454ce388985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5e66528-6298-47ab-9e55-ff0984b40414",
   "metadata": {},
   "source": [
    "# bhubaneswar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9e2adc43-8ed2-444a-8315-86ce65af8f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1,8):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-bhubaneswar-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card clearfix\"):\n",
    "        text=i.find(\"div\",class_=\"m-srp-card__tuple__flex\")\n",
    "        if text:\n",
    "            near.append(text.text)\n",
    "        else:\n",
    "            near.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d7132749-c28e-4f4f-bc2f-c06e770d3645",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhubaneswar=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "bhubaneswar['City_name'] = 'bhubaneswar'\n",
    "\n",
    "bhubaneswar.to_csv(\"C://New_PG_Project//New_PG_Project_bhubaneswar_All_Pages.csv\", encoding='utf-8', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ae4500-bf99-4630-8e40-f1381b01919c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a367c6c-39e3-4d9c-9dec-e53061499c2d",
   "metadata": {},
   "source": [
    "# haridwar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a8a9828a-5c40-4d95-9fd3-a49aa3dac491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1,2):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-haridwar-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card clearfix\"):\n",
    "        text=i.find(\"div\",class_=\"m-srp-card__tuple__flex\")\n",
    "        if text:\n",
    "            near.append(text.text)\n",
    "        else:\n",
    "            near.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "781994b8-b449-4f11-8ae3-ead597597f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "haridwar=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "haridwar['City_name'] = 'haridwar'\n",
    "\n",
    "haridwar.to_csv(\"C://New_PG_Project//New_PG_Project_haridwar_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54efea49-9c19-4604-8d13-bf0752ea36e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f859f1-abf6-4936-809b-ab9b4522381e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7a7f6b5-211f-4db4-b958-cf328ab9d479",
   "metadata": {},
   "source": [
    "# mangalore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ccdea13a-7c3e-4837-889c-aba50263f8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1,4):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-mangalore-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card clearfix\"):\n",
    "        text=i.find(\"div\",class_=\"m-srp-card__tuple__flex\")\n",
    "        if text:\n",
    "            near.append(text.text)\n",
    "        else:\n",
    "            near.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6308209c-10da-45f6-8634-3e9a42a1b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mangalore=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "mangalore['City_name'] = 'mangalore'\n",
    "\n",
    "mangalore.to_csv(\"C://New_PG_Project//New_PG_Project_mangalore_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b688650f-8f8f-4b9a-9b96-3435b160bfe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f32e50-cc55-43e1-a5aa-31de8bfaf089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dc2b139-d09b-4d0f-a565-0e0782436458",
   "metadata": {},
   "source": [
    "# varanasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3b7c0de8-b0cf-4505-ba73-8b7b6a38cad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1,6):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-varanasi-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card clearfix\"):\n",
    "        text=i.find(\"div\",class_=\"m-srp-card__tuple__flex\")\n",
    "        if text:\n",
    "            near.append(text.text)\n",
    "        else:\n",
    "            near.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "756d39a5-7c23-441e-9226-4f8159c13814",
   "metadata": {},
   "outputs": [],
   "source": [
    "varanasi=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "varanasi['City_name'] = 'varanasi'\n",
    "\n",
    "varanasi.to_csv(\"C://New_PG_Project//New_PG_Project_varanasi_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374ef396-1aa2-4b6d-b4cd-0afb1488cbd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4140710b-0550-4b99-b454-764afe9dfc7c",
   "metadata": {},
   "source": [
    "# Indore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a601d961-37dc-4142-9e3a-56967ff2d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Title=[]\n",
    "Type=[]\n",
    "Single_Share=[]\n",
    "Single_Share_AC=[]\n",
    "Twin_Share_AC=[]\n",
    "Twin_Share=[]\n",
    "Triple_Share=[]\n",
    "Triple_Share_AC=[]\n",
    "Four_Share=[]\n",
    "Four_Share_AC=[]\n",
    "location=[]\n",
    "near=[]\n",
    "PG_Availables=[]\n",
    "Title_Based_Price=[]\n",
    "gender=[]\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1,9):\n",
    "    url = f\"https://www.magicbricks.com/pg-in-indore-pppfr/page-{page}\"\n",
    "    headers= {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"span\",class_=\"m-srp-card__title__name\"):\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo__room-detail clearfix\"):\n",
    "    #     Type.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__roomInfo\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"Single.Room\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share.append(regex[0])   \n",
    "        else:\n",
    "            Single_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Single.Room.With\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Single_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Single_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share_AC.append(np.nan)\n",
    "            \n",
    "    \n",
    "        regex=re.findall(r\"Twin\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Twin_Share.append(regex[0])\n",
    "        else:\n",
    "            Twin_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Triple\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Triple_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Triple_Share_AC.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share.append(regex[0])\n",
    "        else:\n",
    "            Four_Share.append(np.nan)\n",
    "    \n",
    "        regex=re.findall(r\"Four\\sSharing\\sWith\\sAC\\s.(\\d+.\\d+)\",text)\n",
    "                        \n",
    "        if regex:\n",
    "            Four_Share_AC.append(regex[0])\n",
    "        else:\n",
    "            Four_Share_AC.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__link m-srp-card__link--nearby\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card clearfix\"):\n",
    "        text=i.find(\"div\",class_=\"m-srp-card__tuple__flex\")\n",
    "        if text:\n",
    "            near.append(text.text)\n",
    "        else:\n",
    "            near.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\", class_=\"m-srp-card__key-f srp-card-dflex\"):\n",
    "        if i:\n",
    "            PG_Availables.append(i.text.strip() if i.text.strip() else np.nan)\n",
    "        else:\n",
    "            PG_Availables.append(np.nan)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"price-preferred clearfix\"):\n",
    "        Title_Based_Price.append(i.text)\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",class_=\"m-srp-card__info flex__item\"):\n",
    "        text=i.text\n",
    "        regex=re.findall(r\"^\\w+\",text)\n",
    "        if regex:\n",
    "            gender.append(regex[0])\n",
    "        else:\n",
    "            gender.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bb39a24f-629c-4e34-8cda-b22adb16d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "Indore=pd.DataFrame({\"Title\":Title,\"Title_Based_Price\":Title_Based_Price,\"Living\":gender,\"Single_Share\":Single_Share,\"Single_Share_AC\":Single_Share_AC,\"Twin_Share_AC\":Twin_Share_AC,\"Twin_Share\":Twin_Share,\"Triple_Share\":Triple_Share,\"Triple_Share_AC\":Triple_Share_AC,\n",
    "             \"Four_Share_AC\":Four_Share_AC,\"Four_Share\":Four_Share,\"PG_Availables\":PG_Availables,\"location\":location\n",
    "             })\n",
    "Indore['City_name'] = 'Indore'\n",
    "\n",
    "Indore.to_csv(\"C://New_PG_Project//New_PG_Project_Indore_All_Pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410d2024-a0c9-4783-bda2-6560e80482dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b467e6-4f26-4760-b4d3-84262e8e179d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "99d665d2-42f4-4d00-a45f-1c37646eeb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the path to the folder with CSV files\n",
    "path = \"C:/New_PG_Project/\"\n",
    "\n",
    "# Use glob to get a list of CSV files in the folder\n",
    "all_files = glob.glob(path + \"*.csv\")\n",
    "\n",
    "# Read each CSV file and store them in a list of DataFrames\n",
    "dataframes = [pd.read_csv(file) for file in all_files]\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "NEW_PG_combined_df = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "dfe23112-d4e3-4b1d-a9af-1ed1bda7c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_PG_combined_df.to_csv(\"C://New_PG_Project//ALL_PAGES_DATASET_CITY_WISE_PG_COLLECT.CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eea29b-d047-4147-894b-04ab10052407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1ad4ea89-97e2-459f-94d5-e861714b8f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing values in each column:\n",
      " Unnamed: 0            0.000000\n",
      "Title                 0.000000\n",
      "Title_Based_Price     0.000000\n",
      "Living                0.128188\n",
      "Single_Share         72.364408\n",
      "Single_Share_AC      75.440923\n",
      "Twin_Share_AC        63.868629\n",
      "Twin_Share           58.966538\n",
      "Triple_Share         73.761216\n",
      "Triple_Share_AC      78.362728\n",
      "Four_Share_AC        94.129868\n",
      "Four_Share           92.198205\n",
      "PG_Availables        84.997569\n",
      "location              0.000000\n",
      "City_name             0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "abb_percentage = NEW_PG_combined_df.isnull().mean() * 100\n",
    "\n",
    "# Display the missing value percentages for each column\n",
    "print(\"Percentage of missing values in each column:\\n\", abb_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "953b3b58-03f0-4e12-a6fc-80f8fd4024d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: 193\n",
      "Title_Based_Price: 193\n",
      "Living: 193\n",
      "Single_Share: 193\n",
      "Single_Share_AC: 193\n",
      "Twin_Share_AC: 193\n",
      "Twin_Share: 193\n",
      "Triple_Share: 193\n",
      "Triple_Share_AC: 193\n",
      "Four_Share_AC: 193\n",
      "Four_Share: 193\n",
      "location: 193\n",
      "near: 0\n",
      "PG_Availables: 193\n"
     ]
    }
   ],
   "source": [
    "print(\"Title:\", len(Title))\n",
    "print(\"Title_Based_Price:\", len(Title_Based_Price))\n",
    "print(\"Living:\", len(gender))\n",
    "print(\"Single_Share:\", len(Single_Share))\n",
    "print(\"Single_Share_AC:\", len(Single_Share_AC))\n",
    "print(\"Twin_Share_AC:\", len(Twin_Share_AC))\n",
    "print(\"Twin_Share:\", len(Twin_Share))\n",
    "print(\"Triple_Share:\", len(Triple_Share))\n",
    "print(\"Triple_Share_AC:\", len(Triple_Share_AC))\n",
    "print(\"Four_Share_AC:\", len(Four_Share_AC))\n",
    "print(\"Four_Share:\", len(Four_Share))\n",
    "print(\"location:\", len(location))\n",
    "print(\"near:\", len(near))\n",
    "print(\"PG_Availables:\", len(PG_Availables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a990e0-5602-4847-b599-2b39818aa9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
